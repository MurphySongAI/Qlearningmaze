import numpy as np

# ==========================================
# 0. ã€å…³é”®ä¿®å¤ã€‘è§£å†³ numpy å’Œ gym çš„ç‰ˆæœ¬å†²çª
# ==========================================
# å¦‚æœ numpy ç‰ˆæœ¬è¾ƒæ–°ï¼Œæ‰‹åŠ¨è¡¥ä¸Š bool8 å±æ€§
if not hasattr(np, 'bool8'):
    np.bool8 = np.bool_

import torch
import torch.nn as nn
import torch.optim as optim
import gym
import random
from collections import deque

# ==========================================
# 1. å®šä¹‰ Q ç½‘ç»œ (å¤§è„‘)
# ==========================================
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, x):
        return self.fc(x)

# ==========================================
# 2. å®šä¹‰ DQN æ™ºèƒ½ä½“
# ==========================================
class DQNAgent:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.batch_size = 64
        self.target_update_freq = 10

        self.memory = deque(maxlen=10000) 

        self.q_net = QNetwork(state_dim, action_dim)
        self.target_net = QNetwork(state_dim, action_dim)
        self.target_net.load_state_dict(self.q_net.state_dict())

        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.learning_rate)
        self.loss_func = nn.MSELoss()
        
        self.update_count = 0

    def select_action(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_dim)
        else:
            # ç¡®ä¿ state æ˜¯ tensor å¹¶ä¸”ç»´åº¦æ­£ç¡®
            state = torch.FloatTensor(state)
            if state.dim() == 1:
                state = state.unsqueeze(0)
            
            with torch.no_grad():
                q_values = self.q_net(state)
            return torch.argmax(q_values).item()

    def store_transition(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def learn(self):
        if len(self.memory) < self.batch_size:
            return

        batch = random.sample(self.memory, self.batch_size)
        state, action, reward, next_state, done = zip(*batch)
        
        state = torch.FloatTensor(np.array(state))
        action = torch.LongTensor(action).unsqueeze(1)
        reward = torch.FloatTensor(reward).unsqueeze(1)
        next_state = torch.FloatTensor(np.array(next_state))
        done = torch.FloatTensor(done).unsqueeze(1)

        q_eval = self.q_net(state).gather(1, action)

        with torch.no_grad():
            q_next = self.target_net(next_state).max(1)[0].unsqueeze(1)
        
        q_target = reward + (1 - done) * self.gamma * q_next

        loss = self.loss_func(q_eval, q_target)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
            
        self.update_count += 1
        if self.update_count % self.target_update_freq == 0:
            self.target_net.load_state_dict(self.q_net.state_dict())

# ==========================================
# 3. ä¸»è®­ç»ƒå¾ªç¯ (å…¼å®¹ä¿®å¤ç‰ˆ)
# ==========================================
if __name__ == '__main__':
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make('CartPole-v1')
    
    # è·å–çŠ¶æ€ç»´åº¦ (å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ gym API)
    if hasattr(env.observation_space, 'shape'):
        state_dim = env.observation_space.shape[0]
    else:
        state_dim = 4 # CartPole é»˜è®¤ä¸º 4
        
    action_dim = env.action_space.n
    agent = DQNAgent(state_dim, action_dim)
    
    EPISODES = 200
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ DQN (å·²ä¿®å¤ numpy å’Œ reset é—®é¢˜)...")
    
    for episode in range(EPISODES):
        # --- å…¼å®¹æ€§ä¿®å¤ 1: reset è¿”å›å€¼ ---
        reset_result = env.reset()
        if isinstance(reset_result, tuple):
            state = reset_result[0] # æ–°ç‰ˆ gym
        else:
            state = reset_result    # æ—§ç‰ˆ gym
            
        total_reward = 0
        done = False
        
        while not done:
            action = agent.select_action(state)
            
            # --- å…¼å®¹æ€§ä¿®å¤ 2: step è¿”å›å€¼ ---
            step_result = env.step(action)
            if len(step_result) == 5:
                next_state, reward, terminated, truncated, _ = step_result
                done = terminated or truncated
            else:
                next_state, reward, done, _ = step_result
            
            # å…¼å®¹æ€§ä¿®å¤ 3: æŸäº›ç¯å¢ƒè¿”å›çš„ done æ˜¯ bool ç±»å‹ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯ numpy.bool_
            # è¿™é‡Œç»Ÿä¸€è½¬ä¸º python çš„ boolï¼Œé¿å… tensor æŠ¥é”™
            done = bool(done)

            # ä¿®æ”¹å¥–åŠ±é€»è¾‘ï¼Œæ†å­å€’äº†ç»™æƒ©ç½š
            reward_to_store = reward
            if done and total_reward < 499:
                reward_to_store = -10
            
            agent.store_transition(state, action, reward_to_store, next_state, done)
            agent.learn()
            
            state = next_state
            total_reward += reward
            
            if done:
                print(f"Episode: {episode}, Score: {int(total_reward)}, Epsilon: {agent.epsilon:.2f}")
                
        if total_reward >= 500:
            print(f"âœ… åœ¨ç¬¬ {episode} å±€è§£å†³äº†é—®é¢˜ï¼")
            break
            
    print("è®­ç»ƒç»“æŸï¼")