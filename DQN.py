from sys import modules
import numpy as np

# ==========================================
# 0. ã€å…³é”®ä¿®å¤ã€‘è§£å†³ numpy å’Œ gym çš„ç‰ˆæœ¬å†²çª
# ==========================================
# å¦‚æœ numpy ç‰ˆæœ¬è¾ƒæ–°ï¼Œæ‰‹åŠ¨è¡¥ä¸Š bool8 å±æ€§
if not hasattr(np, 'bool8'):
    np.bool8 = np.bool_

import torch
import torch.nn as nn
import torch.optim as optim
import gym
import random
from collections import deque

# ==========================================
# 1. å®šä¹‰ Q ç½‘ç»œ (å¤§è„‘)
# ==========================================
class QNetwork(nn.Module): 
    # ==========================================
    # ç»§æ‰¿è‡ª nn.Module
    # nn.Module æ˜¯ PyTorch æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å‹çš„åŸºç±»
    # è¯´æ˜è¿™æ˜¯ä¸€ä¸ª ç¥ç»ç½‘ç»œæ¨¡å‹
    # ==========================================
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        # è°ƒç”¨çˆ¶ç±»ï¼ˆnn.Moduleï¼‰çš„åˆå§‹åŒ–å‡½æ•°ï¼Œnn.Module é‡Œé¢åšäº†å¾ˆå¤šé‡è¦çš„åˆå§‹åŒ–å·¥ä½œã€‚
        # å¦‚æœä½ ä¸è°ƒç”¨å®ƒï¼šä½ çš„æ¨¡å‹å°±ä¸æ˜¯ä¸€ä¸ªâ€œçœŸæ­£çš„ PyTorch æ¨¡å‹â€ã€‚
        # super() çš„æ„æ€æ˜¯ï¼šæ‰¾åˆ°çˆ¶ç±»
        # ç­‰ä»·äºsuper().__init__()
        self.fc = nn.Sequential(
            # nn.Sequential ä»£è¡¨æŒ‰é¡ºåºæŠŠå¤šä¸ªå±‚ä¸²èµ·æ¥
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    # ==========================================
    # å‰å‘ä¼ æ’­
    # å®šä¹‰æ¨¡å‹çš„å‰å‘ä¼ æ’­é€»è¾‘ï¼šè¾“å…¥ xï¼Œç»è¿‡å…¨è¿æ¥ç½‘ç»œ self.fcï¼Œè¾“å‡ºç»“æœã€‚
    # ==========================================
    def forward(self, x):
        return self.fc(x)


# ==========================================
# 2. å®šä¹‰ DQN æ™ºèƒ½ä½“
# è´Ÿè´£ä¸‰ä»¶äº‹ï¼š
# 1. é€‰åŠ¨ä½œ
# 2. å­˜ç»éªŒ
# 3. å­¦ä¹ æ›´æ–°ç½‘ç»œ
# ==========================================
class DQNAgent:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim #çŠ¶æ€å‘é‡
        self.action_dim = action_dim #åŠ¨ä½œç©ºé—´
        
        self.gamma = 0.99 #æŠ˜æ‰£å› å­ Q=r+Î³Q(s',a') è¡¨ç¤ºæœªæ¥å¥–åŠ±çš„é‡è¦ç¨‹åº¦ã€‚
        self.epsilon = 1.0 #æ¢ç´¢ç‡
        self.epsilon_min = 0.01 #æœ€å°æ¢ç´¢ç‡
        self.epsilon_decay = 0.995 #æ¢ç´¢ç‡è¡°å‡
        # Îµ-greedy ç­–ç•¥ï¼šæ¢ç´¢ç‡ä»1.0å¼€å§‹ï¼Œéšç€è®­ç»ƒè¿›è¡Œé€æ¸è¡°å‡åˆ°0.01ï¼Œç¡®ä¿åˆæœŸå……åˆ†æ¢ç´¢ï¼ŒåæœŸåˆ©ç”¨å·²æœ‰çŸ¥è¯†ã€‚
        self.learning_rate = 0.001 #å­¦ä¹ ç‡
        self.batch_size = 64 #æ¯æ¬¡è®­ç»ƒé‡‡æ ·64æ¡ç»éªŒ
        self.target_update_freq = 10 #ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡ï¼Œæ¯éš”10æ¬¡è¿­ä»£æ›´æ–°ä¸€æ¬¡ç›®æ ‡ç½‘ç»œï¼Œä¿æŒç›®æ ‡ç½‘ç»œç¨³å®šï¼Œé¿å…éœ‡è¡ã€‚

        self.memory = deque(maxlen=10000) #ç»éªŒå›æ”¾
        # å­˜å‚¨æ•°æ®(state, action, reward, next_state, done)ï¼Œæœ€å¤š10000æ¡

        self.q_net = QNetwork(state_dim, action_dim) #å½“å‰è®­ç»ƒçš„ç½‘ç»œ
        self.target_net = QNetwork(state_dim, action_dim) #å›ºå®šä¸€æ®µæ—¶é—´çš„â€œç¨³å®šç›®æ ‡ç½‘ç»œâ€
        # æ ¸å¿ƒæ˜¯ä¸¤ä¸ªç½‘ç»œ
        self.target_net.load_state_dict(self.q_net.state_dict()) #ä¸€å¼€å§‹ä¸¤ä¸ªç½‘ç»œå‚æ•°ç›¸åŒã€‚

        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.learning_rate)
        self.loss_func = nn.MSELoss()
        # ä¼˜åŒ–å™¨ï¼šAdam ä¼˜åŒ–å™¨ï¼Œç”¨äºæ›´æ–° q_net çš„å‚æ•°ã€‚
        # æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·® (MSE)ï¼Œç”¨äºè¡¡é‡é¢„æµ‹å€¼ä¸ç›®æ ‡å€¼ä¹‹é—´çš„å·®è·ã€‚loss=ï¼ˆQ_eval-Q_targetï¼‰^2
        
        self.update_count = 0

    def select_action(self, state):
        # é€‰åŠ¨ä½œ ä»¥epsilonçš„æ¦‚ç‡éšæœºé€‰ï¼Œå¦åˆ™ä»¥1-epsilonçš„æ¦‚ç‡é€‰Qå€¼æœ€å¤§çš„åŠ¨ä½œ
        # æœ€å¼€å§‹çš„æ¢ç´¢ç‡æœ€å¤§
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_dim)
        else:
            # ç¡®ä¿ state æ˜¯ tensor å¹¶ä¸”ç»´åº¦æ­£ç¡®
            state = torch.FloatTensor(state)
            if state.dim() == 1:
                state = state.unsqueeze(0)
            
            with torch.no_grad():
                q_values = self.q_net(state)
            return torch.argmax(q_values).item()

    def store_transition(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
        # å¾€ç»éªŒæ± ä¸­æ·»åŠ ä¸€æ¡ç»éªŒ

    def learn(self):
        if len(self.memory) < self.batch_size:
            return
        # å¦‚æœç»éªŒæ± ä¸­çš„ç»éªŒå°‘äºbatch_sizeï¼Œä¸è¿›è¡Œå­¦ä¹ ï¼Œä¸è®­ç»ƒï¼Œä¸æ›´æ–°ç½‘ç»œ
        # ä¼šå…ˆå¼€å§‹éšæœºè¡ŒåŠ¨ï¼Œæ”¶é›†ç»éªŒï¼Œå­˜å…¥memoryï¼Œç­‰memoryå¤§äº64çš„æ—¶å€™ï¼Œå†å¼€å§‹å­¦ä¹ 

        batch = random.sample(self.memory, self.batch_size)
        # ä»ç»éªŒæ± ä¸­éšæœºé‡‡æ ·batch_size = 64æ¡ç»éªŒ
        state, action, reward, next_state, done = zip(*batch)
        
        state = torch.FloatTensor(np.array(state))
        action = torch.LongTensor(action).unsqueeze(1)
        reward = torch.FloatTensor(reward).unsqueeze(1)
        next_state = torch.FloatTensor(np.array(next_state))
        done = torch.FloatTensor(done).unsqueeze(1)

        q_eval = self.q_net(state).gather(1, action)

        with torch.no_grad():
            q_next = self.target_net(next_state).max(1)[0].unsqueeze(1)
        
        q_target = reward + (1 - done) * self.gamma * q_next
        # è´å°”æ›¼æ–¹ç¨‹ Q(s,a) = r + gamma * max Q(s',a')

        loss = self.loss_func(q_eval, q_target)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        # åå‘ä¼ æ’­ï¼Œæ›´æ–°ç½‘ç»œå‚æ•°ï¼šæ¸…ç©ºæ¢¯åº¦ï¼Œè®¡ç®—æ¢¯åº¦ï¼Œæ›´æ–°å‚æ•°

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
            # æ¢ç´¢ç‡è¡°å‡
            
        self.update_count += 1
        if self.update_count % self.target_update_freq == 0:
            self.target_net.load_state_dict(self.q_net.state_dict())

        return loss.item()

# ==========================================
# 3. ä¸»è®­ç»ƒå¾ªç¯ (å…¼å®¹ä¿®å¤ç‰ˆ)
# ==========================================
if __name__ == '__main__':
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make('CartPole-v1')
    
    # è·å–çŠ¶æ€ç»´åº¦ (å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ gym API)
    if hasattr(env.observation_space, 'shape'):
        state_dim = env.observation_space.shape[0]
    else:
        state_dim = 4 # CartPole é»˜è®¤ä¸º 4
        
    action_dim = env.action_space.n
    agent = DQNAgent(state_dim, action_dim)
    
    EPISODES = 200
    
    # Set up logging
    log_f = open("dqn_training_log.txt", "w", encoding='utf-8')
    def log(msg):
        print(msg)
        log_f.write(str(msg) + "\n") # Ensure msg is converted to string
        log_f.flush()

    log("ğŸš€ å¼€å§‹è®­ç»ƒ DQN (å·²ä¿®å¤ numpy å’Œ reset é—®é¢˜)...")
    
    for episode in range(EPISODES):
        # --- å…¼å®¹æ€§ä¿®å¤ 1: reset è¿”å›å€¼ ---
        reset_result = env.reset()
        if isinstance(reset_result, tuple):
            state = reset_result[0] # æ–°ç‰ˆ gym
        else:
            state = reset_result    # æ—§ç‰ˆ gym
            
        total_reward = 0
        done = False
        step_count = 0
        log(f"\n=== Episode {episode} Start ===")
        
        while not done:
            action = agent.select_action(state)
            
            # --- å…¼å®¹æ€§ä¿®å¤ 2: step è¿”å›å€¼ ---
            step_result = env.step(action)
            if len(step_result) == 5:
                next_state, reward, terminated, truncated, _ = step_result
                done = terminated or truncated
            else:
                next_state, reward, done, _ = step_result
            
            # å…¼å®¹æ€§ä¿®å¤ 3: æŸäº›ç¯å¢ƒè¿”å›çš„ done æ˜¯ bool ç±»å‹ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯ numpy.bool_
            # è¿™é‡Œç»Ÿä¸€è½¬ä¸º python çš„ boolï¼Œé¿å… tensor æŠ¥é”™
            done = bool(done)

            # ä¿®æ”¹å¥–åŠ±é€»è¾‘ï¼Œæ†å­å€’äº†ç»™æƒ©ç½š
            reward_to_store = reward
            if done and total_reward < 499:
                reward_to_store = -10
            
            # --- è®°å½•æ—¥å¿— ---
            # 1. è·å–å½“å‰çŠ¶æ€çš„ Q å€¼ (ä»…ç”¨äºå±•ç¤º)
            state_tensor = torch.FloatTensor(state)
            if state_tensor.dim() == 1:
                state_tensor = state_tensor.unsqueeze(0)
            
            with torch.no_grad():
                q_values_log = agent.q_net(state_tensor).detach().numpy().flatten()
                q_values_str = "[" + ", ".join([f"{q:.3f}" for q in q_values_log]) + "]"

            # 2. æ‰§è¡Œå­¦ä¹ å¹¶è·å– Loss
            agent.store_transition(state, action, reward_to_store, next_state, done)
            loss = agent.learn()
            
            # 3. æ‰“å°è¯¦ç»†æ—¥å¿—
            loss_str = f"{loss:.5f}" if loss is not None else "N/A"
            log(f"Step: {step_count:3d} | State: {np.round(state, 2)} | Q-values: {q_values_str} | "
                  f"Action: {action} | Reward: {reward:.1f} | Loss: {loss_str} | Epsilon: {agent.epsilon:.3f}")
            
            state = next_state
            total_reward += reward
            step_count += 1
            
            if done:
                log(f"Episode: {episode}, Score: {int(total_reward)}, Epsilon: {agent.epsilon:.2f}")
                log("-" * 100)
                
        if total_reward >= 500:
            log(f"âœ… åœ¨ç¬¬ {episode} å±€è§£å†³äº†é—®é¢˜ï¼")
            break
            
    log("è®­ç»ƒç»“æŸï¼")
    log_f.close()
    torch.save(agent.q_net, "DQN_model.pth")