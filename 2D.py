import numpy as np
import time
import os
import random

# ==========================================
# 1. å®šä¹‰è¿·å®«ç¯å¢ƒ (Maze Environment)
# ==========================================
class MazeEnv:
    def __init__(self):
        # 5x5 åœ°å›¾è®¾è®¡
        # S:èµ·ç‚¹, T:ç»ˆç‚¹, #:å¢™å£(ä¸é€š), .:è·¯, O:é™·é˜±(é€šä½†æ‰£åˆ†)
        self.map = [
            ['S', '.', '.', '#', '.'],
            ['.', '#', '.', '#', '.'],
            ['.', '#', '.', '.', '.'],
            ['.', '.', 'O', '#', '.'],
            ['.', '.', '.', '#', 'T']
        ]
        self.n_rows = 5
        self.n_cols = 5
        self.robot_pos = (0, 0) # èµ·å§‹ä½ç½®
        self.target_pos = (4, 4)
        
        # åŠ¨ä½œç©ºé—´ï¼šä¸Š(0), ä¸‹(1), å·¦(2), å³(3)
        self.action_space = ['u', 'd', 'l', 'r']
        self.n_actions = 4

    def reset(self):
        self.robot_pos = (0, 0)
        return self.robot_pos

    def step(self, action):
        x, y = self.robot_pos
        
        # å³ä½¿æ’å¢™ï¼ŒåŸæ¥çš„ä½ç½®ä¹Ÿä¸èƒ½å˜ï¼Œå…ˆå­˜ä¸€ä¸‹
        next_x, next_y = x, y
        
        # å°è¯•ç§»åŠ¨
        if action == 0:   # Up
            next_x = max(0, x - 1)
        elif action == 1: # Down
            next_x = min(self.n_rows - 1, x + 1)
        elif action == 2: # Left
            next_y = max(0, y - 1)
        elif action == 3: # Right
            next_y = min(self.n_cols - 1, y + 1)

        # æ£€æŸ¥æ˜¯å¦æ’å¢™ (#)
        if self.map[next_x][next_y] == '#':
            # æ’å¢™äº†ï¼ä½ç½®ä¿æŒä¸å˜ï¼Œç»™ä¸ªæƒ©ç½š
            reward = -5
            done = False
            next_state = (x, y) # å¼¹å›åŸåœ°
        else:
            # ç§»åŠ¨æˆåŠŸ
            self.robot_pos = (next_x, next_y)
            next_state = (next_x, next_y)
            
            # åˆ¤æ–­å½“å‰ä½ç½®çš„å¥–åŠ±
            cell_type = self.map[next_x][next_y]
            
            if cell_type == 'T':    # åˆ°è¾¾ç»ˆç‚¹
                reward = 50
                done = True
            elif cell_type == 'O':  # æ‰è¿›é™·é˜±
                reward = -20
                done = False
            else:                   # æ™®é€šè·¯é¢
                reward = -1         # æ¯èµ°ä¸€æ­¥æ‰£1åˆ†ï¼Œå¼ºè¿«å®ƒæ‰¾æœ€çŸ­è·¯å¾„
                done = False
                
        return next_state, reward, done

    def render(self):
        # ç®€å•çš„æ–‡æœ¬å¯è§†åŒ–
        # os.system('cls' if os.name == 'nt' else 'clear') # å¦‚æœæƒ³æ¸…å±å¯ä»¥å–æ¶ˆæ³¨é‡Š
        print("-" * 20)
        for i in range(self.n_rows):
            row_str = ""
            for j in range(self.n_cols):
                if (i, j) == self.robot_pos:
                    row_str += "ğŸ¤– " # æœºå™¨äººå½“å‰ä½ç½®
                elif self.map[i][j] == '#':
                    row_str += "â¬› " # å¢™å£
                elif self.map[i][j] == 'T':
                    row_str += "ğŸ " # ç»ˆç‚¹
                elif self.map[i][j] == 'O':
                    row_str += "âŒ " # é™·é˜±
                else:
                    row_str += "â¬œ " # è·¯
            print(row_str)
        print("-" * 20)

# ==========================================
# 2. Q-Learning æ™ºèƒ½ä½“
# ==========================================
class QLearningAgent:
    def __init__(self, n_rows, n_cols, n_actions):
        self.n_rows = n_rows
        self.n_cols = n_cols
        self.n_actions = n_actions
        # åˆå§‹åŒ– Q è¡¨ï¼š5x5x4 çš„ä¸‰ç»´æ•°ç»„
        self.q_table = np.zeros((n_rows, n_cols, n_actions))
        
        self.lr = 0.1       # å­¦ä¹ ç‡ Alpha
        self.gamma = 0.9    # æŠ˜æ‰£å› å­ Gamma
        self.epsilon = 0.1  # æ¢ç´¢ç‡ Epsilon

    def choose_action(self, state, is_training=True):
        # Epsilon-Greedy ç­–ç•¥
        if is_training and np.random.uniform() < self.epsilon:
            return np.random.choice(self.n_actions) # éšæœºæ¢ç´¢
        else:
            x, y = state
            # å³ä½¿ Q å€¼éƒ½ä¸€æ ·ï¼Œä¹Ÿéšæœºé€‰ä¸€ä¸ªï¼Œé˜²æ­¢æ­»æ¿
            state_action = self.q_table[x, y, :]
            # æ‰¾åˆ°æœ€å¤§å€¼çš„ç´¢å¼•ï¼ˆå¦‚æœæœ‰å¤šä¸ªæœ€å¤§å€¼ï¼Œéšæœºé€‰ä¸€ä¸ªï¼‰
            max_indices = np.where(state_action == np.max(state_action))[0]
            return np.random.choice(max_indices)

    def learn(self, state, action, reward, next_state, done):
        x, y = state
        nx, ny = next_state
        
        q_predict = self.q_table[x, y, action]
        
        if done:
            q_target = reward
        else:
            q_target = reward + self.gamma * np.max(self.q_table[nx, ny, :])
            
        # æ›´æ–°å…¬å¼
        self.q_table[x, y, action] += self.lr * (q_target - q_predict)

# ==========================================
# 3. ä¸»ç¨‹åºï¼šè®­ç»ƒ + æ¼”ç¤º
# ==========================================
if __name__ == "__main__":
    env = MazeEnv()
    agent = QLearningAgent(env.n_rows, env.n_cols, env.n_actions)
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ™ºèƒ½ä½“...")
    
    # --- è®­ç»ƒé˜¶æ®µ ---
    EPISODES = 500
    for episode in range(EPISODES):
        state = env.reset()
        done = False
        
        while not done:
            action = agent.choose_action(state)
            next_state, reward, done = env.step(action)
            agent.learn(state, action, reward, next_state, done)
            state = next_state

    print("âœ… è®­ç»ƒå®Œæˆï¼ç°åœ¨æ¼”ç¤ºæ™ºèƒ½ä½“çš„èµ°æ³•ï¼š\n")
    time.sleep(1)

    # --- æ¼”ç¤ºé˜¶æ®µ (å¯è§†åŒ–) ---
    state = env.reset()
    done = False
    step_count = 0
    
    env.render() # æ‰“å°åˆå§‹çŠ¶æ€
    
    while not done:
        time.sleep(0.5) # æš‚åœ0.5ç§’è®©ä½ çœ‹æ¸…æ¥šæ¯ä¸€æ­¥
        
        # è¿™ä¸€æ­¥å®Œå…¨æŒ‰ç…§å­¦åˆ°çš„ Q è¡¨èµ° (ä¸æ¢ç´¢)
        action = agent.choose_action(state, is_training=False)
        state, reward, done = env.step(action)
        
        env.render()
        step_count += 1
        
        if step_count > 20: # é˜²æ­¢æ­»å¾ªç¯ï¼ˆå¦‚æœæ²¡è®­ç»ƒå¥½çš„è¯ï¼‰
            print("è¿·è·¯äº†...")
            break
            
    if done:
        print(f"ğŸ‰ æˆåŠŸæŠµè¾¾ç»ˆç‚¹ï¼å…±ç”¨äº† {step_count} æ­¥ã€‚")